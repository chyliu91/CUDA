## 全局内存的合并与非合并访问
从费米架构开始，有了SM层次的L1缓存和设备层次的L2缓存，可以用于缓存全局内存的访问。在启用了L1缓存的情况下，对全局内存的读取将首先尝试经过L1缓存；如果未命中，则接着尝试经过L2缓存；如果再次未命中，则直接从DRAM读取。一次数据传输处理的数据量在默认情况下是32字节。

关于全局内存的访问模式，有合并（coalesced）与非合并（uncoalesced）之分。合并访问指的是一个线程束对全局内存的一次访问请求（读或者写）导致最少数量的数据传输，否则称访问是非合并的。定量地说，可以定义一个合并度（degree of coalescing），它等于线程束请求的字节数除以由该请求导致的所有数据传输处理的字节数。如果所有数据传输中处理的数据都是线程束所需要的，那么合并度就是100%，即对应合并访问。所以，也可以将合并度理解为一种资源利用率。利用率越高，核函数中与全局内存访问有关的部分的性能就更好；利用率低则意味着对显存带宽的浪费。

为简单起见，我们主要以全局内存的读取和仅使用L2缓存的情况为例进行下述讨论。在此情况下，一次数据传输指的就是将32字节的数据从全局内存（DRAM）通过32字节的L2缓存片段（cache sector）传输到SM。

考虑一个线程束访问单精度浮点数类型的全局内存变量的情形。因为一个单精度浮点数占有4字节，故该线程束将请求128字节的数据。在理想情况下（即合并度为100%的情况），仅触发128/32=4次用L2缓存的数据传输。那么，在什么情况下会导致多于4次数据传输呢？

在一次数据传输中，从全局内存转移到L2缓存的一片内存的首地址一定是一个最小粒度（这里是32字节）的整数倍。例如，一次数据传输只能从全局内存读取地址为0～31字节、32～63字节、64～95字节、96～127字节等片段的数据。如果线程束请求的全局内存数据的地址刚好为0~127字节或者128～255字节等，就能与4次数据传输所处理的数据完全吻合。这种情况下的访问就是合并访问。

如何保证一次数据传输中内存片段的首地址为最小粒度的整数倍呢？或者问：如何控制所使用的全局内存的地址呢？答案是，使用CUDA运行时API函数（如我们常用的cudaMalloc）分配的内存的首地址至少是256字节的整数倍。

顺序的合并访问:

```
void global_add(float* x, float* y, float* z)
{
    int n = threadIdx.x + blockIdx.x * blockDim.x;
    z[n] = x[n] + y[n];
}
global_add<<<128, 32>>>(x, y, z);
```
其中，x、y和z是由cudaMalloc()分配全局内存的指针。很容易看出，核函数中对这几个指针所指内存区域的访问都是合并的。例如，第一个线程块中的线程束将访问数组x中第0~31个元素，对应128字节的连续内存，而且首地址一定是256字节的整数倍。这样的访问只需要4次数据传输即可完成，所以是合并访问，合并度为100%。


乱序的合并访问:

```
void global_add_permuted(float *x, float *y, float *z)
{
    int tid_permuted = threadIdx.x ^ 0x1; // 0x1 是某种置换操作
    int n = tid_permuted + blockIdx.x * blockDim.x;
    z[n] = x[n] + y[n];
}
global_add_permuted<<<128, 32>>>(x, y, z);
```
其中，`threadIdx.x ^ 0x1` 是某种置换操作，作用是将0~31的整数做某种置换（交换两个相邻的数）。第一个线程块中的线程束将依然访问数组x中第0~31个元素，只不过线程号与数组元素指标不完全一致而已。这样的访问是乱序的（或者交叉的）合并访问，合并度也为100%。

不对齐的非合并访问:
```
void global_add_offset(float *x, float *y, float *z)
{
    int n = threadIdx.x + blockIdx.x * blockDim.x + 1;
    z[n] = x[n] + y[n];
}
global_add_offset<<<128, 32>>>(x, y, z);
```

第一个线程块中的线程束将访问数组x中第1～32个元素。假如数组x的首地址为256字节，该线程束将访问设备内存的260～387字节。这将触发5次数据传输，对应的内存地址分别是256~287字节、288~319字节、320~351字节、352~383字节和384~415字节。这样的访问属于不对齐的非合并访问，合并度为4/5×100%=80%。


跨越式的非合并访问:

```
void global_add_stride(float *x, float *y, float *z)
{
    int n = blockIdx.x + threadIdx.x * gridDim.x;
    z[n] = x[n] + y[n];
}
global_add_stride<<<128, 32>>>(x, y, z);
```

第一个线程块中的线程束将访问数组x中指标为0、128、256、384等的元素。因为这里的每一对数据都不在一个连续的32字节的内存片段，故该线程束的访问将触发32次数据传输。这样的访问属于跨越式的非合并访问，合并度为4/32×100%=12.5%。


广播式的非合并访问:
```
void global_add_broadcast(float *x, float *y, float *z)
{
    int n = threadIdx.x + blockIdx.x * blockDim.x;
    z[n] = x[0] + y[n];
}
global_add_broadcast<<<128, 32>>>(x, y, z);
```

第一个线程块中的线程束将一致地访问数组x中的第0个元素。这只需要一次数据传输（处理32字节的数据），但由于整个线程束只使用了4字节的数据，故合并度为4/32×100%=12.5%。这样的访问属于广播式的非合并访问。


## 使用全局内存实现矩阵转置
合并读实现：
```
__global__ void transpose1(const double *A, double *B, const int N)
{
	const int nx = blockIdx.x * blockDim.x + threadIdx.x;
	const int ny = blockIdx.y * blockDim.y + threadIdx.y;
	if (nx < N && ny < N)
	{
		B[nx * N + ny] = A[ny * N + nx];
	}
}
```

合并写实现：
```
__global__ void transpose2(const double *A, double *B, const int N)
{
	const int nx = blockIdx.x * blockDim.x + threadIdx.x;
	const int ny = blockIdx.y * blockDim.y + threadIdx.y;
	if (nx < N && ny < N)
	{
		B[ny * N + nx] = A[nx * N + ny];
	}
}
```
以上两个核函数中都有一个合并访问和一个非合并访问，在核函数 `transpose2()` 中，读取操作虽然是非合并的，但利用了只读数据缓存的加载函数 `__ldg()`。从帕斯卡架构开始，如果编译器能够判断一个全局内存变量在整个核函数的范围都只可读（如这个例子中的矩阵A），则会自动用函数 `__ldg()` 读取全局内存，从而对数据的读取进行缓存，缓解非合并访问带来的影响。对于全局内存的写入，则没有类似的函数可用。这就是以上两个核函数性能差别的根源。所以，在不能满足读取和写入都是合并的情况下，一般来说应当尽量做到合并地写入。

对于开普勒架构和麦克斯韦架构，默认情况下不会使用 `__ldg()` 函数。因此需要明显地使用 `__ldg()` 函数:
```
__global__ void transpose3(const double *A, double *B, const int N)
{
    const int nx = blockIdx.x * blockDim.x + threadIdx.x;
    const int ny = blockIdx.y * blockDim.y + threadIdx.y;
    if (nx < N && ny < N)
    {
        B[ny * N + nx] = __ldg(&A[nx * N + ny]);
    }
}
```
除了利用只读数据缓存加速非合并的访问外，有时还可以利用共享内存将非合并的全局内存访问转化为合并的。









